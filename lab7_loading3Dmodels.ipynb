{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1G42yM3b_N9eetDq0-UXXfyTQfDvSemgT",
      "authorship_tag": "ABX9TyOIKOxm3sCSuaU1btxF02wy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hallpaz/3dsystems23/blob/main/lab7_loading3Dmodels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3D Graphics Systems | AI Graphics - Theory and Practice | IMPA 2023\n",
        "### Instructor: Luiz Velho\n",
        "### TA: Hallison Paz\n",
        "### Course info: https://lvelho.impa.br/i3d23/\n",
        "\n",
        "## Lab Class #7 - Loading and Visualizing 3D Data"
      ],
      "metadata": {
        "id": "99h1P9wRlt-7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94d5XQfCxjnK"
      },
      "source": [
        "# Part 1 - Read and elaborate\n",
        "\n",
        "1.1 Read the paper [ShapeNet: An Information-Rich 3D Model Repository](https://arxiv.org/abs/1512.03012). What can you say about the importance and the challenges of building a large dataset for 3D machine learning?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bWGn109zHeX"
      },
      "source": [
        "*Use this space to write your thoughts*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FOLccI7zN4a"
      },
      "source": [
        "# Part 2 - Learn\n",
        "\n",
        "2.0 Run the following cells and read the explanations. It's an opportunity to get some familiarization with PyTorch3D. In \"part 3\" you are going to run some experiments on the ShapeNet dataset, and in \"part 4\", you are required to load models from another dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxk0Z2D1MOVn"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith((\"1.13.\", \"2.0.\")) and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Oo6esYBN0EM"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from pytorch3d.datasets import (\n",
        "    R2N2,\n",
        "    ShapeNetCore,\n",
        "    collate_batched_meshes,\n",
        "    render_cubified_voxels,\n",
        ")\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PointLights,\n",
        "    RasterizationSettings,\n",
        "    TexturesVertex,\n",
        "    look_at_view_transform,\n",
        ")\n",
        "\n",
        "from pytorch3d.structures import Meshes\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# add path for demo utils functions \n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZZvt-0oWU3K"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxAa08BZNaF5"
      },
      "source": [
        "If using **Google Colab**, fetch the utils file for plotting image grids and the ShapeNet JSON files to map corresponding ids and categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5PrK6YLNaF5"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/docs/tutorials/utils/plot_image_grid.py\n",
        "\n",
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/pytorch3d/datasets/shapenet/shapenet_synset_dict_v1.json\n",
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/pytorch3d/datasets/shapenet/shapenet_synset_dict_v2.json\n",
        "!cp /content/shapenet_synset_dict_v1.json  /usr/local/lib/python3.6/dist-packages/pytorch3d/datasets/shapenet/shapenet_synset_dict_v1.json\n",
        "!cp /content/shapenet_synset_dict_v2.json  /usr/local/lib/python3.6/dist-packages/pytorch3d/datasets/shapenet/shapenet_synset_dict_v2.json\n",
        "\n",
        "from plot_image_grid import image_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNqnqE-jNaF5"
      },
      "source": [
        "OR if running locally uncomment and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVa9qsEmNaF5"
      },
      "source": [
        "# from utils import image_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URsbxKhNT-At"
      },
      "source": [
        "# 2.1. Load the datasets\n",
        "\n",
        "We'll work on a small subset of the ShapeNetCore version 2 dataset. \n",
        "\n",
        "The whole dataset has more than 25GB and can be downloaded following the instructions here: https://www.shapenet.org/. Check the instructions page for the .zip file with our selected small subset of ShapeNet. If you are running on Google Colab, we suggest you upload the uncompressed folders to you Google Drive account and access its path here.\n",
        "\n",
        "After downloading the data, modify `SHAPENET_PATH` below to you local path to the ShapeNetCore dataset folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-brINtP2T9iD"
      },
      "source": [
        "# Setup\n",
        "if torch.cuda.is_available():\n",
        " device = torch.device(\"cuda:0\")\n",
        " torch.cuda.set_device(device)\n",
        "else:\n",
        " device = torch.device(\"cpu\")\n",
        "\n",
        "SHAPENET_PATH = \"\"\n",
        "shapenet_dataset = ShapeNetCore(SHAPENET_PATH,version=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ_oGjq9NaF7"
      },
      "source": [
        "We can retrieve a model by indexing into the loaded dataset. We can examine the category this model belongs to (in the form of a synset id, equivalent to wnid described in ImageNet's API: http://image-net.org/download-API), its model id, and its vertices and faces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoU8mpPkNaF7"
      },
      "source": [
        "shapenet_model = shapenet_dataset[1]\n",
        "print(\"This model belongs to the category \" + shapenet_model[\"synset_id\"] + \".\")\n",
        "print(\"This model has model id \" + shapenet_model[\"model_id\"] + \".\")\n",
        "model_verts, model_faces = shapenet_model[\"verts\"], shapenet_model[\"faces\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhPBTMg-NaF7"
      },
      "source": [
        "We can use its vertices and faces to form a `Meshes` object which is a PyTorch3D datastructure for working with batched meshes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loIxQLmNNaF7"
      },
      "source": [
        "model_textures = TexturesVertex(verts_features=torch.ones_like(model_verts, device=device)[None])\n",
        "shapenet_model_mesh = Meshes(\n",
        "    verts=[model_verts.to(device)],   \n",
        "    faces=[model_faces.to(device)],\n",
        "    textures=model_textures\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o9STw0VNaF9"
      },
      "source": [
        "## 2.2. Render ShapeNetCore models with PyTorch3D's differentiable renderer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GcfZe-QNaF-"
      },
      "source": [
        "The `ShapeNetCore` dataloader has a customized `render` function that support rendering models by specifying their model ids, categories or indices using PyTorch3D's differentiable renderer implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hewMtu0NNaF-"
      },
      "source": [
        "# Rendering settings.\n",
        "R, T = look_at_view_transform(1.0, 1.0, 90)\n",
        "cameras = FoVPerspectiveCameras(R=R, T=T, device=device)\n",
        "raster_settings = RasterizationSettings(image_size=512)\n",
        "lights = PointLights(location=torch.tensor([0.0, 1.0, -2.0], \n",
        "                                           device=device)[None],device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4jsIteXNaF-"
      },
      "source": [
        "First we will try to render three models by their model ids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrz8EX3kNaF-"
      },
      "source": [
        "images_by_model_ids = shapenet_dataset.render(\n",
        "    model_ids=[\n",
        "        \"1e4df43ee2f2da6967f9cc18b363cf72\",\n",
        "        \"6f6ed9e0d29b64e714be24585075d395\",\n",
        "        \"da0179a5b68f13586a6a687121d74e50\"\n",
        "    ],\n",
        "    device=device,\n",
        "    cameras=cameras,\n",
        "    raster_settings=raster_settings,\n",
        "    lights=lights,\n",
        ")\n",
        "image_grid(images_by_model_ids.cpu().numpy(), rows=1, cols=3, rgb=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDk14yGTNaF-"
      },
      "source": [
        "We can also render models by their indices. For instance, if we would like to render the three models at indices 93, 109 and 200:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRVTVmnxNaF-"
      },
      "source": [
        "images_by_idxs = shapenet_dataset.render(\n",
        "    idxs=[93, 109, 200],\n",
        "    device=device,\n",
        "    cameras=cameras,\n",
        "    raster_settings=raster_settings,\n",
        "    lights=lights,\n",
        ")\n",
        "image_grid(images_by_idxs.cpu().numpy(), rows=1, cols=3, rgb=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLFJN-1fNaF_"
      },
      "source": [
        "Alternatively, if we are not interested in any particular models but would like see random models from some specific categories, we can do that by specifying `categories` and `sample_nums`. For example, if we would like to render 2 models from the category \"mailbox\" and 3 models from the category \"washer\", we can do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4pcdgcrNaF_"
      },
      "source": [
        "# we also have the category \"remote\" in this subset of ShapeNet\n",
        "images_by_categories = shapenet_dataset.render(\n",
        "    categories=[\"mailbox\", \"washer\"],\n",
        "    sample_nums=[2, 3],\n",
        "    device=device,\n",
        "    cameras=cameras,\n",
        "    raster_settings=raster_settings,\n",
        "    lights=lights,\n",
        ")\n",
        "image_grid(images_by_categories.cpu().numpy(), rows=1, cols=5, rgb=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUQd3AqJNaF_"
      },
      "source": [
        "If we are not interested in any particular categories and just would like to render some random models from the whole dataset, we can set the number of models to be rendered in `sample_nums` and not specify any `categories`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVKobMFDNaF_"
      },
      "source": [
        "random_model_images = shapenet_dataset.render(\n",
        "    sample_nums=[5],\n",
        "    device=device,\n",
        "    cameras=cameras,\n",
        "    raster_settings=raster_settings,\n",
        "    lights=lights,\n",
        ")\n",
        "image_grid(random_model_images.cpu().numpy(), rows=1, cols=5, rgb=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XX7c1MUKIYO"
      },
      "source": [
        "# Part 3 - Plotly Visualization\n",
        "\n",
        "PyTorch3D has a differentiable renderer which allows us to generate images and take derivatives of the images with respect to the scene parameters. A differentiable renderer is very powerful, but it more computing intensive and it requires some setup. \n",
        "\n",
        "In this assignment, we only want to visualize the meshes, so we'll use Ploty for that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LnKUtKwRLaJ"
      },
      "source": [
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxzm8eEPY0SI"
      },
      "source": [
        "3.0. Show the number of instances in our subset of the ShapeNet dataset using the function `len` on `shapenet_dataset`\n",
        "\n",
        "3.1. Create a Meshes object by taking it's vertices and faces from a shapenet model. Name your variable `myfirstmesh` so the code on the cell below display correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zju4C1WEZ8nw"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 3.0-3.1.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U-LXy5laYno"
      },
      "source": [
        "# Render the plotly figure\n",
        "fig = plot_scene({\n",
        "    \"subplot1\": {\n",
        "        \"shapenet_mesh\": myfirstmesh\n",
        "    }\n",
        "})\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFwXsFTTbeMS"
      },
      "source": [
        "As You just saw, the function `plot_scene` takes a dictionary of subplots and returns a plotly figure. Each subplot is a dictionary of kind `name: mesh`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayQUgFpfbKCl"
      },
      "source": [
        "3.2. You can render textured models using Ploty. Create a Meshes object passing `verts`, `faces` and `textures` arguments. `textures` must be a `TexturesVertex` object. It's not an easy task to create a `TextureVertex` directly from the ShapeNet model, so for now we suggest you create a simple tensor like a noisy color or other kind of pattern you can control. \n",
        "\n",
        "3.3. Create another Meshes object from ShapeNet and show both models in the same subplot. \n",
        "  \n",
        "  **Extra - E.1: you can create a batch of Meshes instead of two or more individual Meshes objects. To do so, note that the arguments to build a Meshes instance are, in fact, lists.**\n",
        "\n",
        "\n",
        "3.4. Each mesh is represented in its own coordinate system. This way, when you put them in the same subplot, they will overlap. This time, create a new subplot and plot the meshes. \n",
        "\n",
        "  **Extra - E.2: if you created a batch of Meshes, use the function** `plot_batch_individually`.\n",
        "\n",
        "\n",
        "3.5. Experiment changing the figure size using `fig.update_layout(height:int,width:int)` . Experiment adding the arguments below to the `plot_scene` or `plot_batch_individually` functions.\n",
        "```\n",
        "  (...)\n",
        "  xaxis={\"backgroundcolor\":\"rgb(200, 200, 230)\"},\n",
        "  yaxis={\"backgroundcolor\":\"rgb(230, 200, 200)\"},\n",
        "  zaxis={\"backgroundcolor\":\"rgb(200, 230, 200)\"},\n",
        "  axis_args=AxisArgs(showgrid=True)\n",
        "  (...)\n",
        "  ```\n",
        "  **Extra - E.3: Create a `Meshes` instance by computing vertices and faces of a parametric model and visualize it.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Vhkwh_kZpO"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 3.2-3.5 (and extras).\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 - Exploring new data\n",
        "\n",
        "Recently, the Allen Institute for AI released a [dataset called Objaverse](https://objaverse.allenai.org) with 800K+ annotated 3D objects. You may want to check their webpage where you can find a [paper](https://arxiv.org/abs/2212.08051) and a [Google Colab](https://colab.research.google.com/drive/1ZLA4QufsiI_RuNlamKqV7D7mn40FbWoY?usp=sharing) showing how the dataset is structured, and demonstrating how to use it.\n",
        "\n",
        "4.1 Load a couple of 3D objects from the Objaverse dataset as PyTorch3D Meshes and visualize them using PyTorch3D's differentiable renderer. You may need to adjust the camera's or the object's pose for a better visualization. \n",
        "\n",
        "*Note: You will need to enable GLB file format loading as [demonstrated in this file](https://github.com/facebookresearch/pytorch3d/blob/main/pytorch3d/io/experimental_gltf_io.py).*\n",
        "\n",
        "\n",
        "4.2  Now, we want to use the Plotly visualizer to render the objects. Check the `textures` attribute of your `meshes` reference. It's probably a `TexturesUV` instance. As we have learned, Plotly can only render `TexturesVertex` instances. Use the function `convert_to_textureVertex` below to generate a `TexturesVertex` instance and use it as the `textures` attribute of your meshes. Finally, render your meshes using the Plotly backend.\n",
        "\n"
      ],
      "metadata": {
        "id": "It2Ht4nObXK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from: https://github.com/facebookresearch/pytorch3d/issues/854\n",
        "def convert_to_textureVertex(textures_uv: TexturesUV, meshes:Meshes) -> TexturesVertex:\n",
        "    verts_colors_packed = torch.zeros_like(meshes.verts_packed())\n",
        "    verts_colors_packed[meshes.faces_packed()] = textures_uv.faces_verts_textures_packed()\n",
        "    return TexturesVertex(packed_to_list(verts_colors_packed, meshes.num_verts_per_mesh()))"
      ],
      "metadata": {
        "id": "bNh3jPA4bc8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################################################\n",
        "# Code for 4.1-4.2\n",
        "##############################################################################"
      ],
      "metadata": {
        "id": "jE6Za7kkj0H6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}