{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMUkPgOLX5jvnitv6HgITNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hallpaz/3dsystems23/blob/main/assignments/lab10_rendering_textured_pointclouds_and_meshes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lc3EFu_NBJHv"
      },
      "source": [
        "# 3D Graphics Systems | AI Graphics - Theory and Practice | IMPA 2023\n",
        "### Instructor: Luiz Velho\n",
        "### TA: Hallison Paz\n",
        "### Course info: https://lvelho.impa.br/i3d23/\n",
        "\n",
        "## Lab Class #10 - Rendering textured pointclouds and meshes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vm8qcg1BFyZ"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith((\"1.13.\", \"2.0.\")) and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0wJqYghmeNC"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread\n",
        "\n",
        "# Util function for loading meshes\n",
        "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
        "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
        "from pytorch3d.renderer import (\n",
        "    look_at_view_transform,\n",
        "    FoVPerspectiveCameras, \n",
        "    PointLights, \n",
        "    DirectionalLights, \n",
        "    Materials, \n",
        "    RasterizationSettings, \n",
        "    MeshRenderer, \n",
        "    MeshRasterizer,  \n",
        "    SoftPhongShader,\n",
        "    TexturesUV,\n",
        "    TexturesVertex\n",
        ")\n",
        "\n",
        "# add path for demo utils functions \n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVjPXwjNneyF"
      },
      "source": [
        "⚠️ ⚠️ ⚠️  If using **Google Colab**, fetch the utils file for plotting image grids:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjMkgkqHm9hv"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/master/docs/tutorials/utils/plot_image_grid.py\n",
        "from plot_image_grid import image_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p2DtZ25n_5w"
      },
      "source": [
        "# uncomment and run this line if running locally\n",
        "# from utils import image_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYKaWVurn7TX"
      },
      "source": [
        "# Part 1 - Loading a textured mesh and visualizing its texture map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SazwmL9N66Za"
      },
      "source": [
        "0.1 Download a textured mesh set of files. The .obj describes the mesh structure; The .mtl describes its material and the image (.png) is a texture map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTm0cVuOjb1W"
      },
      "source": [
        "!mkdir -p data/cow_mesh\n",
        "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj\n",
        "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.mtl\n",
        "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow_texture.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_iA1h0V_Dor"
      },
      "source": [
        "# Setup\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jsUa8MM7jTZ"
      },
      "source": [
        "1.1 Load the mesh data using the function `load_objs_as_meshes`. It takes a list of paths to one or multiple .obj file and returns a Meshes object. Don't forget to pass the `device` as argument, so you can benefit of the GPU acceleration.\n",
        "\n",
        "1.2 Visualize the texture map loaded with the mesh. You can use the function `texturesuv_image_matplotlib` passing the attribute `textures` of the Meshes instance. There is also a method, texturesuv_image_PIL, to get a similar image which can be saved to a file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTw9seFu-0lT"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 1.1-1.2.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7er4rX27mCgX"
      },
      "source": [
        "PyTorch3D has a built-in way to view the texture map with matplotlib along with the points on the map corresponding to vertices. There is also a method, texturesuv_image_PIL, to get a similar image which can be saved to a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s2UwURf3Hdj"
      },
      "source": [
        "# Part 2 - Rendering a Mesh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcnG6XJ6fBLu"
      },
      "source": [
        "## 2. Create a renderer\n",
        "\n",
        "A renderer in PyTorch3D is composed of a **rasterizer** and a **shader** which each have a number of subcomponents such as a **camera** (orthographic/perspective). Here we initialize some of these components and use default values for the rest. We will first create a **renderer** which uses a **perspective camera**, a **point light** and applies **phong shading**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX466mWnfBLv"
      },
      "source": [
        "# Initialize a camera.\n",
        "# With world coordinates +Y up, +X left and +Z in, the front of the cow is facing the -Z direction. \n",
        "# So we move the camera by 180 in the azimuth direction so it is facing the front of the cow. \n",
        "R, T = look_at_view_transform(2.7, 0, 180) \n",
        "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
        "\n",
        "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that \n",
        "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for \n",
        "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of \n",
        "# the difference between naive and coarse-to-fine rasterization. \n",
        "raster_settings = RasterizationSettings(\n",
        "    image_size=512, \n",
        "    blur_radius=0.0, \n",
        "    faces_per_pixel=1, \n",
        ")\n",
        "\n",
        "# Place a point light in front of the object. As mentioned above, the front of the cow is facing the \n",
        "# -z direction. \n",
        "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
        "\n",
        "# Create a phong renderer by composing a rasterizer and a shader. The textured phong shader will \n",
        "# interpolate the texture uv coordinates for each vertex, sample from a texture image and \n",
        "# apply the Phong lighting model\n",
        "renderer = MeshRenderer(\n",
        "    rasterizer=MeshRasterizer(\n",
        "        cameras=cameras, \n",
        "        raster_settings=raster_settings\n",
        "    ),\n",
        "    shader=SoftPhongShader(\n",
        "        device=device, \n",
        "        cameras=cameras,\n",
        "        lights=lights\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyOY5qXvfBLz"
      },
      "source": [
        "## 3. Render the mesh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VkRA4qJfBL0"
      },
      "source": [
        "2.0 Run the following cell to use the `renderer` function to render the previously loaded mesh. It takes a `Meshes` object and returns a tensor that you can interpret as an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBLZH8iUfBL1"
      },
      "source": [
        "images = renderer(mesh)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.grid(\"off\");\n",
        "plt.axis(\"off\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGeHBJxAGFwT"
      },
      "source": [
        "from pytorch3d.renderer import ( \n",
        "    SoftGouraudShader,\n",
        "    SoftSilhouetteShader,\n",
        "    HardFlatShader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I0Cr3IkBM-P"
      },
      "source": [
        "2.1 Move the light by changing its location coordinates and render the scene again. You don't need to recreate the renderer to specify a new light setting. When you call your renderer, you can pass a **lights parameter**: `renderer(mesh, lights=newlights)`\n",
        "\n",
        "\n",
        "2.2 Experiment to define a new renderer using the `HardFlatShader` and render the mesh. \n",
        "\n",
        "2.3 - We can also change the material of a batch of meshes. Define a new `Material` object and experiment changing the **material reflectance** properties of the mesh by specifying new values for the attributes `specular_color` and `shininess`. Again, you can pass a materials parameter as you call your renderer: `renderer(mesh, material=newmaterial)`\n",
        "\n",
        "\n",
        "#### **You are going to learn more about illumination models in a few weeks. For now, just try to run these experiments to get used to the APIs available in PyTorch3D and write your comments based on your observations.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6z7n-klQmWP"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 2.1-2.3.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyOGu1IzNm_J"
      },
      "source": [
        "### Extra\n",
        "E.1 Compute the estimated normals for each vertex of the mesh and render a normal map using the `SoftGouraudShader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94WP_D_tRuLt"
      },
      "source": [
        "##############################################################################\n",
        "# Code for E.1\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsArx7xYOLDv"
      },
      "source": [
        "## Parte 3 - Moving the scene"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8b_vH85UdnY"
      },
      "source": [
        "from pytorch3d.transforms import Transform3d, Rotate, Translate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf3QyEowG_eK"
      },
      "source": [
        "\n",
        "\n",
        "3.1 Check the documentation for the function `look_at_view_transform`. It gives us an intuitive way to define a virtual camera, returning a rotation matrix and translation vector to be applied on the scene. Define a new camera and render the scene from another viewpoint by passing the parameter `cameras` to your renderer: `renderer(mesh, cameras=newcamera)` .\n",
        "\n",
        "3.2 In addition to moving the camera, we can move the object too, by applying a transformation to its vertices. Rotate the mesh by 90 degrees around the vertical axis (Y) and render the scene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K2lRh52YxC7"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 3.1-3.2.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17c4xmtyfBMH"
      },
      "source": [
        "## Part 4. Batched Rendering\n",
        "\n",
        "One of the core design choices of the PyTorch3D API is to support **batched inputs for all components**. \n",
        "The renderer and associated components can take batched inputs and **render a batch of output images in one forward pass**. We will now use this feature to render the mesh from many different viewpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkNW8O9Rnnom"
      },
      "source": [
        "3.0 Run the following cells and observe how the operations are executed in batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDQKebNNfBMI"
      },
      "source": [
        "# Set batch size - this is the number of different viewpoints from which we want to render the mesh.\n",
        "batch_size = 20\n",
        "\n",
        "# Create a batch of meshes by repeating the cow mesh and associated textures. \n",
        "# Meshes has a useful `extend` method which allows us do this very easily. \n",
        "# This also extends the textures. \n",
        "meshes = mesh.extend(batch_size)\n",
        "\n",
        "# Get a batch of viewing angles. \n",
        "elev = torch.linspace(0, 180, batch_size)\n",
        "azim = torch.linspace(-180, 180, batch_size)\n",
        "\n",
        "# All the cameras helper methods support mixed type inputs and broadcasting. So we can \n",
        "# view the camera from the same distance and specify dist=2.7 as a float,\n",
        "# and then specify elevation and azimuth angles for each viewpoint as tensors. \n",
        "R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
        "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
        "\n",
        "# Move the light back in front of the cow which is facing the -z direction.\n",
        "lights.location = torch.tensor([[0.0, 0.0, -3.0]], device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyYJCwEDfBML"
      },
      "source": [
        "# We can pass arbitrary keyword arguments to the rasterizer/shader via the renderer\n",
        "# so the renderer does not need to be reinitialized if any of the settings change.\n",
        "images = renderer(meshes, cameras=cameras, lights=lights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTy9nrc5mCgd"
      },
      "source": [
        "image_grid(images.cpu().numpy(), rows=4, cols=5, rgb=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_lyXn7Qpd2I"
      },
      "source": [
        "4.2 Create a batch of 2 meshes and use the function `image_grid` to show two different images so that the leftmost should be facing the right direction, while the rightmost should be facing the left direction.\n",
        "\n",
        "4.1 Use any configuration of camera and object transformations to render **a single image** of a scene with two equal objects facing each other with some space between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zn6UtaJqM-7"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 4.1-4.2.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y5kHagne2xO"
      },
      "source": [
        "## Part 5. Rendering Point Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYBjT-m0rRHF"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from pytorch3d.structures import Pointclouds\n",
        "from pytorch3d.renderer import (\n",
        "    FoVOrthographicCameras, \n",
        "    PointsRasterizationSettings,\n",
        "    PointsRenderer,\n",
        "    PulsarPointsRenderer,\n",
        "    PointsRasterizer,\n",
        "    AlphaCompositor,\n",
        "    NormWeightedCompositor\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnXNIUM2r5VJ"
      },
      "source": [
        "### Load a point cloud and corresponding colors\n",
        "\n",
        "Load and create a **Point Cloud** object. \n",
        "\n",
        "**Pointclouds** is a unique datastructure provided in PyTorch3D for working with batches of point clouds of different sizes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiJNIKenr7kL"
      },
      "source": [
        "If running this notebook using **Google Colab**, run the following cell to fetch the pointcloud data and save it at the path `data/PittsburghBridge`:\n",
        "If running locally, the data is already available at the correct path. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7a__-znfZLi"
      },
      "source": [
        "!mkdir -p data/PittsburghBridge\n",
        "!wget -P data/PittsburghBridge https://dl.fbaipublicfiles.com/pytorch3d/data/PittsburghBridge/pointcloud.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiWnps4rrpqk"
      },
      "source": [
        "# Set paths\n",
        "DATA_DIR = \"./data\"\n",
        "obj_filename = os.path.join(DATA_DIR, \"PittsburghBridge/pointcloud.npz\")\n",
        "# Load point cloud and colors\n",
        "pointcloud = np.load(obj_filename)\n",
        "verts = torch.Tensor(pointcloud['verts']).to(device)        \n",
        "rgb = torch.Tensor(pointcloud['rgb']).to(device)\n",
        "point_cloud = Pointclouds(points=[verts], features=[rgb])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORmVI2FTCvgM"
      },
      "source": [
        "# Initialize a camera.\n",
        "R, T = look_at_view_transform(20, 10, 0)\n",
        "cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n",
        "\n",
        "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
        "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
        "# and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters. \n",
        "raster_settings = PointsRasterizationSettings(\n",
        "    image_size=512, \n",
        "    radius = 0.003,\n",
        "    points_per_pixel = 10\n",
        ")\n",
        "\n",
        "\n",
        "# Create a points renderer by compositing points using an alpha compositor (nearer points\n",
        "# are weighted more heavily).\n",
        "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
        "pcrenderer = PointsRenderer(\n",
        "    rasterizer=rasterizer,\n",
        "    compositor=AlphaCompositor()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVASWYslCvgM"
      },
      "source": [
        "images = pcrenderer(point_cloud)\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.grid(\"off\")\n",
        "plt.axis(\"off\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo5v18nHtDH6"
      },
      "source": [
        "We will now modify the **renderer** to use **weighted compositing** with a set background color. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYGTaVa0tN_6"
      },
      "source": [
        "pcrenderer = PointsRenderer(\n",
        "    rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "    # Pass in background_color to the norm weighted compositor, setting the background color \n",
        "    # to the 3 item tuple, representing rgb on a scale of 0 -> 1, in this case red\n",
        "    compositor=NormWeightedCompositor(background_color=(1,0,0))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxpzmx5ytbCC"
      },
      "source": [
        "5.1 Render and display a new image of the PointCloud.\n",
        "\n",
        "5.2 Experiment changing the background color"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqdNAQcdt3iE"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 5.1-5.2.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wx23gjgquGpS"
      },
      "source": [
        "## Using the pulsar backend\n",
        "\n",
        "Switching to the pulsar backend is easy! The pulsar backend has a compositor built-in, so the `compositor` argument is not required when creating it (a warning will be displayed if you provide it nevertheless). It pre-allocates memory on the rendering device, that's why it needs the `n_channels` at construction time.\n",
        "\n",
        "All parameters for the renderer forward function are batch-wise except the background color (in this example, `gamma`) and you have to provide as many values as you have examples in your batch. The background color is optional and by default set to all zeros. You can find a detailed explanation of how gamma influences the rendering function here in the paper [Fast Differentiable Raycasting for Neural Rendering using\n",
        "Sphere-based Representations](https://arxiv.org/pdf/2004.07484.pdf).\n",
        "\n",
        "You can also use the `native` backend for the pulsar backend which already provides access to point opacity. The native backend can be imported from `pytorch3d.renderer.points.pulsar`; you can find examples for this in the folder `docs/examples`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8FicR9YuH8j"
      },
      "source": [
        "pcrenderer = PulsarPointsRenderer(\n",
        "    rasterizer=PointsRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
        "    n_channels=4\n",
        ").to(device)\n",
        "\n",
        "images = pcrenderer(point_cloud, gamma=(1e-4,),\n",
        "                  bg_col=torch.tensor([0.0, 1.0, 0.0, 1.0], dtype=torch.float32, device=device))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
        "plt.grid(\"off\")\n",
        "plt.axis(\"off\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQlI5PrMum-1"
      },
      "source": [
        "5.3 Create a batch of pointclouds and render the scene with different viewpoints and background colors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niFT_7UI0HEM"
      },
      "source": [
        "##############################################################################\n",
        "# Code for 5.1-5.2.\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}